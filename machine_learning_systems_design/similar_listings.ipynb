{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Design\n",
    "- **Recognizing constraints**: we probably want sub 100ms latency for the similarity retrieval, there is an order of magnitude 10M listings\n",
    "- **Database**: always provide schemas for each database, and mention what is the primary key / partition keys / clustering keys. Partition key defines which node the data is on, clustering key defines what the data is ordered by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Model\n",
    "- **Consider KNN**: Inference is O(N*d), with N being the number of training data in KNN and d being the dimension of the embedding in each data. This is too slow. We can store the top K neighbors for each listing in a separate database, but it would need to be updated when new listings are added. This is suboptimal.\n",
    "- **Consider Clustering**: Pre-compute the centroids of clusters; during inference time, compute similarities with items within the cluster. Cluster size is crutial -> if there are k items in each cluster, the inference runtime would be O(k*d) with d being the dimension of the vectors. When a new item is added, we compare it with the centroids to classify it into a cluster. Every X period of time, we redo the clustering and re-compute the centroids. If we store similar listings to a particular listing in a DB, it also would need to be updated, which wouldn't work very well.\n",
    "    - K-Means: require knowing the number clusers, globular clusters, simple and fast\n",
    "    - DBSCAN: arbitrary number of clusters with arbitrary shape, doesn't provide centroids but you can compute post-hoc\n",
    "\n",
    "- **Create Embedding Model via Triplet Loss**: Create a model that maps similar listings close to each other in embedding space. Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def generate_example_data(num_samples, input_dim):\n",
    "    anchors = np.random.rand(num_samples, input_dim)\n",
    "    positives = anchors + 0.1 * np.random.randn(num_samples, input_dim)\n",
    "    negatives = np.random.rand(num_samples, input_dim)\n",
    "    return anchors, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors, positives, negatives = generate_example_data(10, 10)\n",
    "anchors.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor.grad = tensor([-0.5567,  0.4102, -0.3252, -0.9785])\n",
      "positive.grad = tensor([0.9110, 0.1212, 0.3342, 0.2091])\n",
      "negative.grad = tensor([-0.3543, -0.5314, -0.0090,  0.7694])\n"
     ]
    }
   ],
   "source": [
    "# define triplet loss function using pytorch: https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "triplet_loss_fn = nn.TripletMarginLoss()\n",
    "\n",
    "# example\n",
    "anchor = torch.randn(4, requires_grad=True)\n",
    "positive = torch.randn(4, requires_grad=True)\n",
    "negative = torch.randn(4, requires_grad=True)\n",
    "output = triplet_loss_fn(anchor, positive, negative)\n",
    "output.backward() # need requires_grad for all tensors, results gets\n",
    "\n",
    "print(f\"anchor.grad = {anchor.grad}\")\n",
    "print(f\"positive.grad = {positive.grad}\")\n",
    "print(f\"negative.grad = {negative.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an MLP as the embedding model. The output size would be the dimension of the embeddings that's genreated. This model takes in some input vector with dimension = `input_size` and outputs an embedding with dimension = `output_size`. The hidden layers would have a dimension in the `hidden_sizes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_sizes: list[int], output_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # constructs the layers\n",
    "        # Remember, all the common modules / layers support batch computation automatically\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "\n",
    "        # wrap it in nn.Sequential which treats it as a single module, and provides the forward function that passes through all layers\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "        # it takes each layer as its own arguments so you have to unwrap it with *layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test example\n",
    "mlp = MLP(1024, [512, 256, 128, 64], 32)\n",
    "input = torch.randn(1024)\n",
    "out = mlp(input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset class. We don't need a custom collate function because the default collate function stacks the tensors, and that's what we want\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Every custom dataset must implement __init__, __len__, and __getitem__\n",
    "    \"\"\"\n",
    "    def __init__(self, anchors, positives, negatives):\n",
    "        self.anchors = anchors\n",
    "        self.positives = positives\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.anchors) == len(self.positives)\n",
    "        assert len(self.anchors) == len(self.negatives)\n",
    "        return len(self.anchors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.anchors[idx], self.positives[idx], self.negatives[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 1024\n",
    "embedding_dim = 32\n",
    "anchors, positives, negatives = generate_example_data(1000, feature_dim)\n",
    "\n",
    "# convert them to float 32 because model weights are by default float 32. Otherwise, cannot do computations of different dtypes\n",
    "anchors = torch.from_numpy(anchors).float()\n",
    "positives = torch.from_numpy(positives).float()\n",
    "negatives = torch.from_numpy(negatives).float()\n",
    "\n",
    "dataset = TripletDataset(anchors, positives, negatives)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "mlp = MLP(feature_dim, [512, 256, 128, 64], embedding_dim)\n",
    "triplet_loss_fn = nn.TripletMarginLoss()\n",
    "optimizer = torch.optim.Adam(params=mlp.parameters(), lr=1e-4) # adam optimizer is optimizing over the model parameters, so you must pass in params\n",
    "\n",
    "def train(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_anchors, batch_positives, batch_negatives in dataloader:\n",
    "            optimizer.zero_grad() # always necessary at each iteration\n",
    "            anchor_embeddings, positive_embeddings, negative_embeddings = mlp(batch_anchors), mlp(batch_positives), mlp(batch_negatives)\n",
    "            loss = triplet_loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() # .item() gets the scalar from a tensor\n",
    "\n",
    "        print(f\"Epoch {epoch}: total loss = {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: total loss = 30.614742279052734\n",
      "Epoch 1: total loss = 28.332452535629272\n",
      "Epoch 2: total loss = 20.422190502285957\n",
      "Epoch 3: total loss = 7.288619086146355\n",
      "Epoch 4: total loss = 2.1929321084171534\n"
     ]
    }
   ],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
